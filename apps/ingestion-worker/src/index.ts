/**
 * Ingestion Worker
 *
 * Consumes bookmark-ingestion queue and processes bookmarks through a
 * resumable pipeline:
 * 1. Fetches URL, stores markdown, and triggers entity extraction (PENDING → MARKDOWN_READY)
 * 2. Transitions status (→ CONTENT_READY) - summary is generated by entity-worker
 * 3. Creates text chunks (→ CHUNKS_READY)
 * 4. Generates embeddings (→ DONE)
 *
 * On retry, the worker resumes from the last completed stage based on status.
 */

import pLimit from "p-limit";

import type { Env, BookmarkIngestionMessage } from "./types.js";
import { isRetryableError } from "./errors.js";
import {
  createDb,
  BookmarkRepository,
  ChunkRepository,
  createEmbeddingProvider,
  fetchAndConvertToMarkdown,
  chunkMarkdown,
  generateEmbeddings,
  type Chunk,
  type BookmarkStatus,
  type EmbeddingProvider,
} from "@rag-bookmarks/shared";

type StepName = "fetch" | "summarize" | "chunk" | "embed";

type StageError = {
  stage?: StepName;
  code?: string;
  status?: number;
};

interface PipelineStage {
  fromStatus: BookmarkStatus;
  step: StepName;
}

interface PipelineContext {
  bookmarkId: string;
  url: string;
  userId: string;
  env: Env;
  bookmarkRepo: BookmarkRepository;
  chunkRepo: ChunkRepository;
  embeddingProvider: EmbeddingProvider;

  markdown: string | null;
  title: string | null;
  storedChunks: Chunk[];
  entitiesExtracted: boolean;
}

function parsePositiveInt(value: string | undefined, fallback: number): number {
  const parsed = Number.parseInt(value ?? "", 10);
  if (Number.isNaN(parsed) || parsed <= 0) {
    return fallback;
  }
  return parsed;
}

function annotateStage(error: unknown, stage: StepName): void {
  if (error && typeof error === "object") {
    (error as StageError).stage = stage;
  }
}

function getErrorContext(error: unknown): StageError | null {
  if (error && typeof error === "object") {
    return error as StageError;
  }
  return null;
}

const PIPELINE: PipelineStage[] = [
  { fromStatus: "PENDING", step: "fetch" },
  {
    fromStatus: "MARKDOWN_READY",
    step: "summarize",
  },
  { fromStatus: "CONTENT_READY", step: "chunk" },
  { fromStatus: "CHUNKS_READY", step: "embed" },
];

const stepExecutors: Record<StepName, (ctx: PipelineContext) => Promise<void>> =
  {
    async fetch(ctx: PipelineContext): Promise<void> {
      let markdown = ctx.markdown;
      let title = ctx.title;
      let metadata:
        | {
            description?: string | undefined;
            favicon?: string | undefined;
            ogImage?: string | undefined;
          }
        | undefined;

      if (!markdown || !title) {
        const fetched = await fetchAndConvertToMarkdown(ctx.url);
        markdown = fetched.markdown;
        title = fetched.title;
        metadata = fetched.metadata;

        const markdownChanged = ctx.markdown !== markdown;

        await ctx.bookmarkRepo.update({
          id: ctx.bookmarkId,
          title,
          ...(metadata?.description && { description: metadata.description }),
          ...(metadata?.favicon && { favicon: metadata.favicon }),
          ...(metadata?.ogImage && { ogImage: metadata.ogImage }),
          markdown,
          ...(markdownChanged ? { entitiesExtracted: false } : {}),
        });

        ctx.markdown = markdown;
        ctx.title = title;
        if (markdownChanged) {
          ctx.entitiesExtracted = false;
        }
      }

      if (!markdown || !title) {
        throw new Error("Missing markdown or title after fetch step");
      }

      const shouldEnqueueEntities = !ctx.entitiesExtracted;
      if (shouldEnqueueEntities) {
        await ctx.env.ENTITY_QUEUE.send({
          type: "entity-extraction",
          bookmarkId: ctx.bookmarkId,
          userId: ctx.userId,
        });
      }

      await ctx.bookmarkRepo.update({
        id: ctx.bookmarkId,
        status: "MARKDOWN_READY",
      });

      console.log(
        `Bookmark ${ctx.bookmarkId}: Fetched and stored markdown (${markdown.length} chars)${
          shouldEnqueueEntities ? ", enqueued entity extraction" : ""
        }`
      );
    },

    async summarize(ctx: PipelineContext): Promise<void> {
      // Summary is now generated by entity-worker along with entity extraction
      // This step just transitions status to allow chunking to proceed
      await ctx.bookmarkRepo.update({
        id: ctx.bookmarkId,
        status: "CONTENT_READY",
      });

      console.log(
        `Bookmark ${ctx.bookmarkId}: Transitioned to CONTENT_READY (summary handled by entity-worker)`
      );
    },

    async chunk(ctx: PipelineContext): Promise<void> {
      if (!ctx.markdown) {
        throw new Error("Missing markdown for chunk step");
      }

      await ctx.chunkRepo.deleteByBookmarkId(ctx.bookmarkId);

      const textChunks = chunkMarkdown(ctx.markdown);

      if (textChunks.length > 0) {
        const chunkParams = textChunks.map((chunk) => ({
          bookmarkId: ctx.bookmarkId,
          content: chunk.content,
          position: chunk.position,
          tokenCount: chunk.tokenCount,
          breadcrumbPath: chunk.breadcrumbPath,
        }));

        ctx.storedChunks = await ctx.chunkRepo.createMany(chunkParams);
      } else {
        ctx.storedChunks = [];
      }

      await ctx.bookmarkRepo.update({
        id: ctx.bookmarkId,
        status: "CHUNKS_READY",
      });

      console.log(
        `Bookmark ${ctx.bookmarkId}: Created and stored ${ctx.storedChunks.length} chunks`
      );
    },

    /**
     * Generate embeddings for chunks that don't have them
     */
    async embed(ctx: PipelineContext): Promise<void> {
      const chunksNeedingEmbeddings =
        await ctx.chunkRepo.findChunksWithoutEmbeddings(ctx.bookmarkId);

      if (chunksNeedingEmbeddings.length === 0) {
        await ctx.bookmarkRepo.update({ id: ctx.bookmarkId, status: "DONE" });
        console.log(
          `Bookmark ${ctx.bookmarkId}: All chunks already have embeddings, marked DONE`
        );
        return;
      }

      const chunkContents = chunksNeedingEmbeddings.map((c) => c.content);

      console.log(
        `Bookmark ${ctx.bookmarkId}: Generating embeddings for ${chunkContents.length} chunks`
      );

      const embeddings = await generateEmbeddings(
        chunkContents,
        ctx.embeddingProvider
      );

      const updates = chunksNeedingEmbeddings
        .map((chunk, index) => {
          const embedding = embeddings[index];
          if (!embedding) {
            return null;
          }
          return { id: chunk.id, embedding };
        })
        .filter((update): update is { id: string; embedding: number[] } =>
          Boolean(update)
        );

      if (updates.length > 0) {
        await ctx.chunkRepo.updateEmbeddings(updates);
      }

      await ctx.bookmarkRepo.update({ id: ctx.bookmarkId, status: "DONE" });

      console.log(
        `Bookmark ${ctx.bookmarkId}: Stored embeddings for ${embeddings.length} chunks`
      );
    },
  };

async function handleIngestionMessage(
  message: BookmarkIngestionMessage,
  env: Env,
  bookmarkRepo: BookmarkRepository,
  chunkRepo: ChunkRepository
): Promise<void> {
  const { bookmarkId, url, userId } = message;
  console.log(`Processing bookmark ${bookmarkId}: ${url}`);

  try {
    const bookmark = await bookmarkRepo.findById(bookmarkId);
    if (!bookmark) {
      console.log(`Bookmark ${bookmarkId} not found, skipping`);
      return;
    }

    if (bookmark.status === "DONE" || bookmark.status === "FAILED") {
      console.log(
        `Bookmark ${bookmarkId}: Already ${bookmark.status.toLowerCase()}, skipping`
      );
      return;
    }

    const existingChunks = await chunkRepo.findByBookmarkId(bookmarkId);

    const ctx: PipelineContext = {
      bookmarkId,
      url,
      userId,
      env,
      bookmarkRepo,
      chunkRepo,
      embeddingProvider: createEmbeddingProvider(
        "jina",
        env.JINA_API_KEY,
        "jina-embeddings-v3",
        "retrieval.passage"
      ),
      markdown: bookmark.markdown,
      title: bookmark.title,
      storedChunks: existingChunks,
      entitiesExtracted: bookmark.entitiesExtracted,
    };

    const startIdx = PIPELINE.findIndex(
      (stage) => stage.fromStatus === bookmark.status
    );
    const actualStartIdx = startIdx >= 0 ? startIdx : 0;

    console.log(
      `Bookmark ${bookmarkId}: Status is ${bookmark.status}, resuming from stage ${actualStartIdx} (${PIPELINE[actualStartIdx]?.step})`
    );

    for (let i = actualStartIdx; i < PIPELINE.length; i++) {
      const stage = PIPELINE[i];
      if (!stage) continue;

      try {
        await stepExecutors[stage.step](ctx);
      } catch (error) {
        annotateStage(error, stage.step);
        throw error;
      }
    }

    console.log(`Bookmark ${bookmarkId} processed successfully`);
  } catch (error) {
    const errorMessage =
      error instanceof Error ? error.message : "Unknown error";
    const context = getErrorContext(error);

    if (isRetryableError(error)) {
      throw error;
    }

    await bookmarkRepo.update({
      id: bookmarkId,
      status: "FAILED",
      errorMessage,
    });

    console.error("Non-retryable ingestion error:", {
      bookmarkId,
      error,
      stage: context?.stage,
      status: context?.status,
      code: context?.code,
    });
  }
}

export default {
  async queue(
    batch: MessageBatch<BookmarkIngestionMessage>,
    env: Env
  ): Promise<void> {
    console.log(
      `Processing batch of ${batch.messages.length} messages from queue: ${batch.queue}`
    );

    const { db, close } = createDb(env.DATABASE_URL);
    const bookmarkRepo = new BookmarkRepository(db);
    const chunkRepo = new ChunkRepository(db);

    const limit = pLimit(parsePositiveInt(env.QUEUE_CONCURRENCY, 2));
    const baseDelaySeconds = parsePositiveInt(env.RETRY_BASE_DELAY_SECONDS, 2);
    const maxDelaySeconds = parsePositiveInt(env.RETRY_MAX_DELAY_SECONDS, 60);

    try {
      await Promise.all(
        batch.messages.map((message) =>
          limit(async () => {
            const { bookmarkId } = message.body;
            try {
              await handleIngestionMessage(
                message.body,
                env,
                bookmarkRepo,
                chunkRepo
              );
              message.ack();
            } catch (error) {
              const isRetryable = isRetryableError(error);
              const context = getErrorContext(error);

              if (!isRetryable) {
                console.error("Failed to process ingestion message:", {
                  bookmarkId,
                  error,
                  retryable: false,
                  stage: context?.stage,
                  status: context?.status,
                  code: context?.code,
                });

                message.ack();
                return;
              }

              const attempts = message.attempts ?? 1;
              const delay = Math.min(
                baseDelaySeconds * 2 ** (attempts - 1),
                maxDelaySeconds
              );

              console.error("Retryable ingestion error:", {
                bookmarkId,
                attempt: attempts,
                retryDelay: delay,
                error,
                retryable: true,
                stage: context?.stage,
                status: context?.status,
                code: context?.code,
              });

              message.retry({ delaySeconds: delay });
            }
          })
        )
      );
    } finally {
      await close();
    }
  },
};
